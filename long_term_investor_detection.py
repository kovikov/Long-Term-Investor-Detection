# -*- coding: utf-8 -*-
"""Long- Term-Investor Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kfE7Nif5Pydtswrsev7ovCTb3Cb9OTZH

## 1. Project Title: Long-term Investor Prediction with Machine Learning

While banks are interested in maintaining their customers, they are also interested in getting new customers. In addition, the banks can only really improve their profit margins by encouraging their customers to make long-term deposits and investments with them. One strategy to do this would be to advertise to the customers via telemarketing calls.

This project will aim to employ machine learning for the task of predicting the efficacy of telemarketing campaigns run by banking institutions.

## 2. Methodology

This project will be carried out using the __CRISP-DM__ methodology. This is one of the more popular data science methodologies and is characterized by six important phases, which are as follows:

1. _Business Understanding_,
2. _Data Understanding_,
3. _Data Preparation_,
4. _Data Modelling_,
5. _Model Evaluation_, and
6. _Model Deployment_.

It should be noted that these phases are usually recurrent in nature (i.e., some phases may be repeated). As such, they do not necessarily follow a linear progression

## 3. Tools

The tools of use for this project include:

1. _Pandas_
2. _NumPy_
3. _Matplotlib_ & _Seaborn_
4. _Sci-kit Learn_

#### 3.1. Pandas & NumPy

__Pandas__ is a Python library built upon the __NumPy__ library. The idea behind _Pandas_ is to be able to operate on text data, where _NumPy_ is best suited for numerical operations, irrespective of the fact that it can represent text to some degree.

#### 3.2. Matplotlib & Seaborn

__Matplotlib__ and __Seaborn__ are Python libraries for data visualization. Other alternatives include __Bokeh__ and __Plotly__.

#### 3.3. Sci-kit Learn

__Sci-kit Learn__ is a Python library for machine learning and data modeling. It provides a lot of utilities and algorithmic implementations for a variety of machine learning tasks such as __classification__, __regression__, __outlier detection__, and __clustering__.

<div align="center"><h1>Project Implementation via CRISP-DM</h1></div>

<div><h3>01. Business Understanding</h3></div>

__EliteBank Investment Services__ is a leading financial institution specializing in investment banking, asset management, and wealth advisory services. To improve their profitability, they intend to run a new telemarketing campaign to bring in fresh investments. To this end, they would like to leverage the data obtained from previous campaigns to forecast how likely they are to convince customers to invest.

This would be an immense boon to them in the following ways:

1. _Provide a likelihood estimation as to the chances of a customer buying long-term deposits_.
2. _Provide good plan as to the customers to focus more attention on_.
3. _Provide some information as to the efficacy of telemarketing campaigns_.

<div><h3>02. Data Understanding</h3></div>

With the _**Business Understanding**_ out of the way, the next step is to understand the data to be obtained and used for the task. This will involve the process of __*Exploratory Data Analysis (EDA)*__.

EDA is the process of sifting through data with the goal of extracting insights. These insights allow a better understanding of the available data and what can be done with it. They can also be used for guided preparation of the dataset in the appropriate manner. Just like regular analysis, EDA begins with a set of __questions__ and/or __hypotheses__. The EDA process will then prove or disprove these hypotheses, and hopefully, reveal other points of inquiry along the way.

The required libraries and packages are imported first. The EDA process is carried out here as shown below. The high-level steps to follow are:

1. Import the required libraries
2. Load in the dataset
3. Analyze and observe its properties.
   * Missing data
   * Inconsistent values
   * Low categorical cardinality
   * Feature correlations
4. Report on these properties and how they might affect our final solution.

<div align="center"><h3>2.1. Enter EDA Code Here</h3></div>
"""

# Import required utilities
import pandas as pd
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt

sns.set()

from scipy.io import arff
import gc

# Seed for reproducibility
SEED = np.random.seed(seed = 2023)

# Import the dataset
data_path = "/content/Bank_Marketing_Dataset.arff"

def truncate_data(data):
    if data.isdigit():
        return int(data)
    else:
        if "." in data:
            try:
                return float(data)
            except:
                return data

    return data

def truncate_row(row):
    return [truncate_data(d) for d in row]

def read_data(path, num_lines=500000*4):
    with open(path, "r") as f:
        lines = f.readlines()

    ix = 0
    for (i, line) in enumerate(lines):
        if line.__contains__("@DATA"):
            ix = i
            break
    lines = lines[:num_lines+ix+1]

    header = [l.split(" ")[1] for l in lines[:ix] if l.startswith("@ATTRIBUTE")]

    data = [truncate_row(l.replace("\n", "").strip().split(",")) for l in lines[ix+1:]]
    data = [l for l in data if len(l) == len(header)]

    return pd.DataFrame(data = data, columns = header)

data = read_data(data_path)
data.head()

data.info()

#convert features to correct datatypes
data['age'] = data['age'].astype('int')
data['balance'] = data['balance'].astype('int')
data['day'] = data['day'].astype('int')
data['duration'] = data['duration'].astype('int')
data['campaign'] = data['campaign'].astype('int')
data['pdays'] = data['pdays'].astype('int')
data['previous'] = data['previous'].astype('int')

data.info()

# Feature cardinality
data.nunique()

data.shape

"""#### Data summary"""

# Describe data
data.describe(include="all")

"""##### Missing Values"""

# Check for missing values
100 * data.isnull().sum() / len(data)

"""Based on the results of the short data analysis above, it would seem that there are no missing values.

#### Customer ages and jobs

It makes some sense to observe the range of __ages__ and __jobs__ represented in the dataset. This can be done by observing the distribution of the __age__ and __job__ variables.
"""

# Distribution of customer ages
sns.displot(data["age"])

plt.title("Distribution of customer ages")

plt.xlabel("Ages (in years)")
plt.ylabel("Age frequency")

plt.show(); plt.close("all")

"""Observing the distribution of customer ages gives the impression that most customers are within their younger years. As such, it is a reasonable expectation that most of the customers to be employed in some gainful manner, as opposed to older, senior citizens, who are more likely to be retired.

We can confirm this hypothesis by visualizing the distribution of the __job__ feature.
"""

# Occupation distribution
plt.figure(figsize = (20, 10))
sns.countplot(data["job"])

plt.title("Distribution of customer occupations")

plt.xlabel("Occupations")
plt.ylabel("Occupation frequency")

plt.show(); plt.close("all")

"""#### Bivariate observation of the occupations and Age
We can then look at a breakdown of the occupations by age. We will segment the ages into the __working__ and __retired__ classes with a threshold of __60__ years, as 60 is an acceptable retirement age.
"""

# Set age hue
hue = data["age"].apply(lambda x: "Working age" if x < 60 else "Retirement age")

age_job = data[["job"]].copy()
age_job["hue"] = hue.values

age_job.head()

# Occupation distribution based on age
plt.figure(figsize = (15, 8))
sns.countplot(data = age_job, x = "job", hue = "hue", palette = "dark:gold")

plt.title("Distribution of customer occupations")

plt.xlabel("Occupations")
plt.ylabel("Occupation frequency")

plt.legend(["Working class age", "Retirement age"])

plt.show(); plt.close("all")

"""As can be seen from the visualization, most of the occupations are dominated by the young working class, while the retirement sector is dominated by older citizens, lending some credence to the earlier hypothesis.

Also, it is observed that the __unemployed__ segment is completely comprised of young, able-bodied people who should be put to work. As these people have no discernable source of income, __EliteBank__ might be well-advised to pay as little attention as possible to people in this segment when implementing their telemarketing strategies.

In addition, some of the customers were observed to have no records with the bank as regards their source of income (i.e., the __unknown__ segment). It would be a prudent move to rectify this oversight in order to target this set of people more effectively.

#### Customer deposit decision with respect to their loan statuses

The accumulation of debts and/or liabilities may be expected to have some impact on the ability of a customer to invest their money in a venture. Within this dataset, there are three debt types recorded:

1. Debts from personal loans (__loan__),
2. Debts from credit (__default__), and
3. Debts from housing loans (__housing__).

We can observe how these types of debt interrelate, as well as how they relate with the __deposit__ decision of the customers.

<h2 align="center">Interrelation between debt types</h2>

#### Personal loans and credit default status

__Hypothesis__: It would make sense that personal loan and credit are correlated i.e., people who default on credit will also probably default on their personal loans.

To verify our hypothesis, we will compare the __loan__ and __default__ features using a __grouped bar chart__.
"""

# Deposit choice vs. credit defaults
plt.figure(figsize = (12, 5))
sns.countplot(data = data, x = "loan", hue = "default", palette = "dark:silver")

plt.title("Comparison between defaults on personal loans and credit", fontsize = 25)

plt.xlabel("Personal loan defaulted?")
plt.ylabel("Frequency")

# plt.legend(["Working class age", "Retirement age"])

plt.show(); plt.close("all")

"""__Observation__:

From the chart above, it is obvious that:

+ A large majority of the customers have not defaulted on any of the loans, whether personal or credit.
+ Also, although quite a few people have defaulted on personal loans, almost no one has defaulted on credit loans

This might imply one of two things:

+ Most people do not max out their credit line, and/or,
+ Most people do not use credit at all.

#### Personal loans and housing loan default status

__Hypothesis__: It would make sense that personal loan and credit are correlated i.e., people who default on credit will also probably default on their personal loans.

To verify our hypothesis, we will compare the __loan__ and __default__ features using a __grouped bar chart__.
"""

# Personal loans vs. housing loan defaults
plt.figure(figsize = (12, 5))
sns.countplot(data = data, x = "loan", hue = "housing", palette = "dark:silver")

plt.title("Comparison between defaults on personal loans and housing loans", fontsize = 25)

plt.xlabel("Personal loan?")
plt.ylabel("Frequency")

# plt.legend(["Working class age", "Retirement age"])

plt.show(); plt.close("all")

data["housing"].value_counts(normalize = True)

"""__Observation__:

From the chart above, it is obvious that:

+ A large majority of the customers have not defaulted on any personal loans.
+ The likelihood of a customer defaulting on a housing loan is almost random. This may be due to the fact that housing loans usually take longer to pay back, and if you are repaying a loan long enough, there might come a point when a payment default will occur.

This might imply one of two things:

+ Most people do not max out their credit line, and/or,
+ Most people do not use credit at all.

#### Credit and housing loan default status

__Hypothesis__: Housing loans can be quite heavy, and the payback can be taxing. As such, it would make sense that p[eople wih housing loans are more likely to limit their credit loans. If this is so, then there will likely be a lower default rate amongst people with housing loans.

To verify our hypothesis, we will compare the __housing__ and __default__ features using a __grouped bar chart__.
"""

# Personal loans vs. housing loan defaults
plt.figure(figsize = (12, 5))
sns.countplot(data = data, x = "default", hue = "housing", palette = "dark:silver")

plt.title("Comparison between defaults on credit and housing loans", fontsize = 25)

plt.xlabel("Default on credit?")
plt.ylabel("Frequency")

# plt.legend(["Working class age", "Retirement age"])

plt.show(); plt.close("all")

"""__Observation__:

From the chart above, it is obvious that:

+ A large majority of the customers have not defaulted on credit.
+ The likelihood of a customer defaulting on a housing loan is almost random. This may be due to the fact that housing loans usually take longer to pay back, and if you are repaying a loan long enough, there might come a point when a payment default will occur.

<h2 align="center">Relation between debt types and marital status</h2>

#### Customer marital status with respect to their credit default status

__Hypothesis__: Since we know from prior analysis that most customers do not default on credit, we would expect the marital status to have little to no effect on whether or not the customer defaulted.
"""

# Marital status compared to credit default
plt.figure(figsize = (10, 5))
sns.countplot(data = data, x = "marital", hue = "default", palette = "dark:silver")

plt.title("Comparison between marital status and credit default status", fontsize = 25)

plt.xlabel("Marital status")
plt.ylabel("Frequency")

# plt.legend(["Working class age", "Retirement age"])

plt.show(); plt.close("all")

"""__Observation__:

From the chart above, it is obvious that most customers do not default on credit, irrespective of marital status.

#### Customer deposit decision with respect to their housing loan default status

__Hypothesis__: It would make sense to put forward the hypothesis that customers who have defaulted on housing loans previously extended to them may not be in the best financial state to make investment deposits.

We can attempt to observe the veracity of this hypothesis by visualizing the customers' deposit decision based on whether or not they have housing loan defaults. This is done via a __grouped bar chart__.
"""

# Deposit choice vs. credit defaults
plt.figure(figsize = (10, 5))
sns.countplot(data = data, x = "deposit", hue = "default", palette = "dark:silver")

plt.title("Distribution of customer deposit choices by credit default status", fontsize = 25)

plt.xlabel("Deposit Choice")
plt.ylabel("Deposit choice frequency")

# plt.legend(["Working class age", "Retirement age"])

plt.show(); plt.close("all")

"""__Observation__: From the chart above, it is obvious that very few customers have defaulted on their credit. A likely implication of this is that the credit default status of customers will have little to no bearing on whether or not the customer would make a deposit. However, an equally likely, but diametrically opposed) implication is the fact that people who default on credit are likely not to have the funds to make long-term investment deposits anyway.

This is an interesting dichotomy, and might bear more looking into by __EliteBank__.

#### Customer deposit decision with respect to their personal loan default status

__Hypothesis__: It would make sense to put forward the hypothesis that customers who have defaulted on personal loans previously extended to them may not be in the best financial state to make investment deposits.

We can attempt to observe the veracity of this hypothesis by visualizing the customers' deposit decision based on whether or not they have loan defaults. This is done via a __grouped bar chart__.
"""

# Deposit choice vs. loan defaults
plt.figure(figsize = (15, 5))
sns.countplot(data = data, x = "deposit", hue = "loan", palette = "dark:silver")

plt.title("Distribution of customer deposit choices by loan default status", fontsize = 25)

plt.xlabel("Deposit Choice")
plt.ylabel("Deposit choice frequency with respect to loan default")

# plt.legend(["Working class age", "Retirement age"])

plt.show(); plt.close("all")

"""__Observation__: From the chart above, it is obvious that very few customers have defaulted on their credit. A likely implication of this is that the personal loan default status of customers will have some bearing on whether or not the customer would make a deposit. However, an equally likely, but diametrically opposed) implication is the fact that people who default on personal loans are likely not to have the funds to make long-term investment deposits anyway.

This is an interesting dichotomy, and might bear more looking into by __EliteBank__.

#### Customer deposit decision with respect to their housing loan default status

__Hypothesis__: It would make sense to put forward the hypothesis that customers who have defaulted on housing loans previously extended to them may not be in the best financial state to make investment deposits.

We can attempt to observe the veracity of this hypothesis by visualizing the customers' deposit decision based on whether or not they have housing loan defaults. This is done via a __grouped bar chart__.
"""

# Deposit choice vs. housing loans defaults
plt.figure(figsize = (15, 7))
sns.countplot(data = data, x = "deposit", hue = "housing", palette = "dark:silver")

plt.title("Grouped Bar Chart of customer deposit choices by housing loan default status", fontsize = 25)

plt.xlabel("Deposit Choice")
plt.ylabel("Deposit frequency with respect to housing loan default")

# plt.legend(["Working class age", "Retirement age"])

plt.show(); plt.close("all")

"""__Observation__:

From the chart above, it is very obvious that customers who default on housing loans are less likely to make the financial deposit. The segment of people who make the deposit is stringly dominated by people who have not defaulted on housing loans, while the no-deposit segment is inversely dominated by housing loan defaulters.

It may be inferred that this is due to the fact that housing loans are usually quite large. The average housing loan in the U.S., as at __2014__ (the year this dataset was curated) was about __~$ 210,000__ [[Reference](https://www.statista.com/statistics/1320422/mortgage-loan-amount-home-buyers-usa/)], which is a very huge amount of money. Attempting to pay back such expensive mortgage while also shelling out money for investment banking can be a tall order.

All of this may imply that people who are able to more comfortably pay back their loans are more likely to have some extra money for investments. Hence, it might behoove __EliteBank__ to:

+ Subsidize the housing market more effectively for their customers,
+ Buy smart investments that can pay off the mortgage, and/or
+ Provide better systems and strategies for customers to pay back their housing loans

These should help free up more capital on the customers' end for investment.

#### Customer deposit decision with respect to their jobs and educational level

__Hypothesis__: It would make sense to put forward the hypothesis that customers who have higher levels of education would likely have better:
1. Understanding of the investment process, and
2. Better paying jobs, which make them more likely to take advantage of investment services.

We can attempt to observe the veracity of this hypothesis by:

1. Comparing the __job__ with the __education__ level.
2. Comparing the __deposit__ decision with the __education__ level.
3. Comparing the __deposit__ with the __job__ level.

This is done via __grouped bar charts__.
"""

# Deposit choice vs. credit defaults
plt.figure(figsize = (10, 5))
sns.countplot(data = data, x = "deposit", hue = "education", palette = "dark:silver")

plt.title("Distribution of customer deposit choices by education level", fontsize = 25)

plt.xlabel("Deposit Choice")
plt.ylabel("Deposit choice frequency w.r.t. education level")

# plt.legend(["Working class age", "Retirement age"])

plt.show(); plt.close("all")

"""__Observation__: From the chart above, it is obvious that very few customers have defaulted on their credit. A likely implication of this is that the credit default status of customers will have little to no bearing on whether or not the customer would make a deposit. However, an equally likely, but diametrically opposed) implication is the fact that people who default on credit are likely not to have the funds to make long-term investment deposits anyway.

This is an interesting dichotomy, and might bear more looking into by __EliteBank__.

#### Customer job tendencies with respect to their educational level
"""

# Deposit choice vs. education levels
plt.figure(figsize = (10, 5))
sns.countplot(data = data, x = "job", hue = "education",)

plt.title("Distribution of customer deposit choices by education level", fontsize = 25)

plt.xlabel("Deposit Choice")
plt.ylabel("Deposit choice frequency w.r.t. education level")

plt.xticks(rotation = 90)

# plt.legend(["Working class age", "Retirement age"])

plt.show(); plt.close("all")

"""_Observation__: From the chart above, it is difficult to observe any overall trend. However, it would seem that people at the __secondary__ level of education dominate a good number of the occupations. This might make some sense as a lot of people are educated in the present day, and there seems to be a higher premium on education.

However, occupations like __admin__ are dominated by people with __secondary__ education, as these people usually require some degree of expertise (which will usually be learnt in school). People in the __management__ occupation seem to be more likely to have the __tertiary__ level of education. This makes some sense, given that a lot of managerial positions might require __MBA__ degrees.

### Occupation with respect to deposit decision
In addition to the above, the occupation of the customers may be viewed in comparison to their deposit decisions.

__Hypothesis__: Some professions are more likely to be higher paid. As such, people in these professions may likely have more money for the purpose of investing.

We can attempt to verify this by visually juxtaposing the __job__ and __deposit__ variables on a grouped bar chart.
"""

# Deposit choice vs. Occupation
plt.figure(figsize = (15, 7))
sns.countplot(data = data, x = "job", hue = "deposit", palette = "dark:silver")

plt.title("Distribution of customer occupations by deposit choice", fontsize = 25)

plt.xlabel("Occupations")
plt.ylabel("Occupation frequency")

# plt.legend(["Working class age", "Retirement age"])

plt.show(); plt.close("all")

"""Making inferences based on the most recent visualization, it would seem that:

+ __blue-collar__ workers are strongly less likely to make a deposit. Reasons for this are not clear; more data would be required to demystify this observation.

+ The same is observed for the __entrepreneur__ and __self-employed__ segments, although this makes more sense, as entrepreneurs are more likely to need their cash at hand for risk-taking and business expenses. To boost investments from these segments, the team at __EliteBank__ may find it worthwhile to come up with special incentives.

+ Another interesting trend to note is the __retired__ segment. It would seem that retired citizens are quite amenable to making investments. Taken with some real-world knowledge, this trend would make some sense. Younger people can always work to earn more money; older people may not necessarily have that luxury, and as such, have to make their money work for them, which means: __investments__. An appropriate action from __EliteBank__ would be to not only focus on this segment, but also come up with new and improved retirement funds for the elderly. This would incur more profits, with the added benefit of improving their public image.

+ With people in the __management__ segment, they seem to be going neck-to-neck. It is almost a 50-50 luck-of-the-draw. To push things in __EliteBank__'s favour, it might be prudent to do some more customer surveys, while also implementing preliminary incentives and business-oriented advertising to members of this segment. This might encourage them to thie their outfit's financial well-being more closely to __EliteBank__.

#### Customer decision in present campaign vs. their decision in previous campaign

Another avenue of enquiry is relationship between the outcomes of previous campaigns (__poutcome__), and the outcome of the present campaign (__deposit__) recorded in the data. This will allow us see which segment of our customers are likely to be repeat depositors.
"""

# Deposit choice vs. credit defaults
plt.figure(figsize = (10, 7))
sns.countplot(data = data, x = "deposit", hue = "poutcome", palette = "dark:silver")

plt.title("Distribution of customer deposit choices by credit default status", fontsize = 25)

plt.xlabel("Deposit Choice")
plt.ylabel("Deposit choice frequency")

# plt.legend(["Working class age", "Retirement age"])

plt.show(); plt.close("all")

"""__Observation__: The short visualization above would imply that people who made an investment in the previous telemarketing campaign (i.e., the __success__ segment) are more likely than not to make new investments. This is of course, contingent on the bank delivering well; that should be __EliteBank__'s goal, to deliver well, so as to secure more recurring investments.

#### Possible influence of marital Status on deposit choice

__Hypothesis__: Marital status might play some role in the choice of a customer to deposit. Someone who is single would more likely have less responsibility to shoulder, hence possibly freeing them up financially.

We can attempt to verify this by:
1. Observing the customer distribution based on __marital__ status.
2. Compare the __marital__ status to the __deposit__ decision.
"""

### Marital distribution
data["marital"].value_counts()

### Marital distribution (proportion)
data["marital"].value_counts(normalize = True)

"""__Observation__: Based on the above, it is obvious that married people dominate the dataset. As such, the decision made by the married sample, would likely overshadow those by customers with other martital statuses."""

# Deposit choice vs. credit defaults
plt.figure(figsize = (10, 7))
sns.countplot(data = data, x = "deposit", hue = "marital", palette = "dark:silver")

plt.title("Distribution of customer deposit choices by marital status", fontsize = 25)

plt.xlabel("Deposit Choice")
plt.ylabel("Deposit choice frequency")

# plt.legend(["Working class age", "Retirement age"])

plt.show(); plt.close("all")

"""__Observation__:

The short visualization above could be taken in one of three ways. It could imply that:

+ There is no appreciable difference in deposit choice based on marital status. This is due to the fact that although the married people are more likely to make the deposit, than single people, who are in turn more likely to do so than divorced people, the same pattern holds true for the non-depositing segment of the sample. This might imply either that __(a.)__ marital status and the choice to make deposits have little correlation in this dataset or __(b.)__ married people are not well-represented in the data, making it dificult to get a good read on the situation. More data for analysis would be required here.

+ The __married__ and __single__ people are less likely to deposit, while the __divorced__ segment is about even. This inference would need to be exhaustively tested by __EliteBank__.
"""

# Create marital status hash map
m = {
    "married": 0,
    "single": 1,
    "divorced": 2
}

# Obtain correlation between marital status and deposit choice
data.loc[:,["marital"]].map(lambda x: m[x]).corrwith(data["deposit"].apply(lambda x: 1 if x == "yes" else 0), method = "spearman")

"""The correlation results lend some credence to our assumption that marital status and theb success of the campaign are not strongly linked.

#### Customer deposit decision with respect to interval of telemarketing calls

In order to ensure that the customers are not hounded with telemarketing calls, it would make some sense to have a good idea of the optimal interval to make those calls in. Doing this would involve:

1. Knowing the cases of __success__, and
2. understanding the distribution of time passing between the calls made.
"""

# Extract successful segment for this campaign
success = data.loc[data["deposit"] == "yes"]

# Deposit choice vs. number of days between calls
plt.figure(figsize = (10, 5))
sns.histplot(data = success, x = "pdays")

plt.title("Distribution of customer deposit choices by number of days between calls in present campaign", fontsize = 25)

plt.xlabel("Deposit Choice")
plt.ylabel("Number of days between calls (Present campaign)")

# plt.legend(["Working class age", "Retirement age"])

plt.show(); plt.close("all")

"""The histogram generated above is quite telling in its implications. It would seem that an astounding majority of successes are recorded from people who are called as soon as possible between campaigns. From our data dictionary, we know that customers who did not participate in the previous campaign are represented with a __pdays__ value of _-1_. It might be that these people consitue a large portion of the tall peak of our histogram above.

An attempt to support/weaken this argument can be made; visualize the same variables, with the:
+ outcome of the previous campaign (__poutcome__),
+ the number of times the customer was actually contacted in the prevcious campaign (__previous__), and
+ the number of times the customer was contacted in this present campaign (__campaign__)

In addition, we can visualize the distribution of:

+ The number of calls made in the present campaign (__campaign__) which resulted in successful __deposits__ in the present telemarketing campaign.
+ The number of calls made in the previous campaign (__previous__) which resulted in successful __deposits__ in the previous telemarketing campaign.
"""

# Distribution of number of calls in this campaign that ended in successful deposits
plt.figure(figsize = (10, 5))
sns.histplot(data = success, x = "campaign")

plt.title("Distribution of call frequency in present campaign (Successful Deposits)", fontsize = 25)

plt.xlabel("Call frequency across depositing customers")
plt.ylabel("Call frequency")

# plt.legend(["Working class age", "Retirement age"])

plt.show(); plt.close("all")

# Extract successful segment for previous campaign
previous_success = data.loc[data["poutcome"] == "success"]

previous_success

# Distribution of call frequency in previous campaign
plt.figure(figsize = (10, 5))
sns.histplot(data = previous_success, x = "previous",)

plt.title("Distribution of call frequency in previous campaign", fontsize = 25)

plt.xlabel("Number of calls in previous campaign")
plt.ylabel("Call frequency over customers")

# plt.legend(["Working class age", "Retirement age"])

plt.show(); plt.close("all")

"""__Observation__: Taking the last two charts in tandem, it would seem that more calls had to be made in the previous campaign in order to secure deposits, as compared to the present campaign.

#### Inter-campaign call interval vs. previous deposit decision
"""

# Time between inter campaign calls (in days) vs. previous outcome
plt.figure(figsize = (10, 5))
sns.histplot(data = success, x = "pdays", hue = "poutcome")

plt.title("Distribution of number of days before inter-campaign contact by previous outcome", fontsize = 25)

plt.xlabel("Deposit Choice")
plt.ylabel("Deposit choice frequency")

# plt.legend(["Working class age", "Retirement age"])

plt.show(); plt.close("all")

"""Taking these charts in conjunction, it would seem that the most advertising success was made from people who either:
+ had an outcome of __unknown__ from the previous campaign (e.g., were indecisive and decided to think on it and later commit)
+ were fresh customers, with no previous exposure to campaigns with the bank.

This may imply two things:
1. that followup is important with a lot of participants from previous ad campaigns.
2. securing a new customer base and expanding their present one is an important action point.

#### Customer deposit decision and call duration

What is the optimal length of a call? How long, ideally, should the banker keep up the communication attempt?
"""

# Distribution of call durations for successful deposits
plt.figure(figsize = (10, 5))
sns.histplot(data = success, x = "duration",)

plt.title("Distribution of call durations for successful deposits (in seconds)", fontsize = 25)

plt.xlabel("Call durations (in seconds)")
plt.ylabel("Call duration frequency")

plt.show(); plt.close("all")

"""From the chart above, it appears that the most success is observed as soon as possible, but not too soon, at around the _250 second_ (or _~4 minute_) mark. This should give enough time to build rapport with the client, and disemminate necessary information, while not dragging on too long.

#### Label imbalance
Next, an observation of the label distribution can be made. This would allow detection of data imbalance, if there is any.
"""

# Distribution of labels
plt.figure(figsize = (7, 5))
sns.countplot(data = data, x = "deposit",)

plt.title("Label Distribution", fontsize = 25)

plt.xlabel("Label (Deposit?)")
plt.ylabel("Label Frequency")

plt.show(); plt.close("all")

"""It would appear that the label imbalance is slight, and hence negligible. This would imply that the accuracy metric should be trustworthy for evaluating any models built on the dataset.

##### Duplicate records

We check for duplicated rows in the data.
"""

# Get the duplicated records
num_duplicated = len(data.loc[data.duplicated()])

print(f"Number of duplicated records: {num_duplicated}.")

"""There are no duplicated records in the dataset. This is very good.

##### Feature correlation

It would make some sense to observe the features for colinearity. As most of the features in the dataset are numerical, the __Pearson correlation coefficient__ will be utilized.
"""

correlation = data.select_dtypes(exclude=["object"]).corr(method = "pearson")

correlation

# Correlation heat map

plt.figure(figsize = (15, 15/2))

sns.heatmap(correlation, center=.5, annot = True)

plt.title("Correlation heatmap", fontsize = 30, pad = 10)

plt.xticks(fontsize = 20)
plt.yticks(fontsize = 20, rotation = 90*4)

plt.tight_layout()
plt.show(); plt.close("all")
gc.collect()

# Correlation of target variabe with independent variables
target = data["deposit"].apply(lambda x: 1 if x == "yes" else 0)
data.select_dtypes(exclude=["object"]).corrwith(target, method = "spearman")

"""From the visual heatmap above, there is very little multicollinearity in the dataset. Beyond that, it is obvious that most of the features exhibit low correlations with one another and with the target variables themselves. This may be taken as a mix of good and bad news.

The good news is that there is little colinearity among the features. This is good for feature independence, which might imply that each variable encodes information that might be relatively orthogonal to the information encoded within other variables. The bad news is, that any model trained in these features might have a difficult time learning anything from the data, as the target variable has little correlation with the independent variables.

More specifically, the target exhibits the best level of correlation with the:

+ __balance__,
+ __duration__,
+ __pdays__ and
+ __previous__.
"""

data.head()

"""#### 2.2. Data Implications

Implied by our findings above, we can say the following:

1. ___Missing values___: There are no missing values in the dataset.

2. ___Data duplication___: There are no duplicated values in the dataset.

3. ___Correlation___: For the most part, although there are some instances of multicollinearity, the features exhibit very little correlation with one another, hence ensuring feature independence. However, the target variable exhibits a low correlation with the independent variables.

4. ___Categorical features___: There are a few categorical features in the dataset (___job___, ___marital___, ___education___, ___default___, ___housing___, ___loan___, ___contact___, ___poutcome___, and ___deposit___). These will need to be encoded in some way.

<div><h3>03. Data Preparation</h3></div>

Based on the *__Data Implications__* discovered prior, the following steps will be experimented upon for the data preparation stage.

1. __Split Data__: Split the data into train and test sets.
2. __Feature Encoding__: Encode categorical features.
3. __Feature Scaling__: Some features display large magnitudes/ranges (e.g., __balance__ and __duration__). These will need to be scaled.

<div align="center"><h3>3.1. Enter Data Preparation Code Here</h3></div>
"""

from sklearn.model_selection import train_test_split

# Select the column types
scale_columns = [
    "age",
    "balance",
    "day",
    "duration"
]

categorical_columns = data.select_dtypes(include = ["object"]).columns.tolist()
categorical_columns.remove("deposit")

# Extract features and labels from dataset
X, y = data.drop(labels = ["deposit"], axis = 1), data["deposit"]

# Encode labels
map_dictionary = {
    "yes": 1,
    "no": 0
}

y = y.apply(lambda x: map_dictionary[x])

# Separate into train and test splits
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, stratify = y)

"""<div><h3>04. Data Modelling</h3></div>

Three modelling options will be explored:

1. Logistic Regression
2. ExtraTrees
3. CatBoost models

#### 4.1. Modelling
"""

!pip install catboost

# Utilities for data preparation
from sklearn.preprocessing import OrdinalEncoder, StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer

# Utilities for data scaling
from sklearn.pipeline import Pipeline

# Utilities for modeling
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import ExtraTreesClassifier

from catboost import CatBoostClassifier



# Utilities for data preparation
from sklearn.preprocessing import OrdinalEncoder, StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import ExtraTreesClassifier
from catboost import CatBoostClassifier

def get_transformer(categorical_columns, scale_columns, one_hot=True):
    transformers = []
    if categorical_columns:
        if one_hot:
            transformers.append(("onehot", OneHotEncoder(handle_unknown = "ignore"), categorical_columns))
        else:
            transformers.append(("ordinal", OrdinalEncoder(handle_unknown = "use_encoded_value", unknown_value = -1), categorical_columns))
    if scale_columns:
        transformers.append(("scaler", StandardScaler(), scale_columns))

    return ColumnTransformer(transformers=transformers)

# Instantiate models
log_pipe = Pipeline(
    steps = [
        ("1", get_transformer(categorical_columns, scale_columns, one_hot=True)),
        ("2", LogisticRegression(max_iter = 1000))
    ]
)

cat_pipe = Pipeline(
    steps = [
        ("1", get_transformer(categorical_columns, scale_columns, one_hot=False)),
        ("2", CatBoostClassifier(verbose = 0))
    ]
)

extra_pipe = Pipeline(
    steps = [
        ("1", get_transformer(categorical_columns, scale_columns, one_hot=False)),
        ("2", ExtraTreesClassifier())
    ]
)

# Fit ExtraTrees model
extra_pipe.fit(X_train, y_train)

# Fit Linear model
log_pipe.fit(X_train, y_train)

# Fit CatBoost model
cat_pipe.fit(X_train, y_train)

# Fit CatBoost model
cat_pipe.fit(X_train, y_train)

"""<div align="center"><h3>4.1. Enter Data Modelling Code Here</h3></div>

<div><h3>05. Model Evaluation</h3></div>
"""

from sklearn.metrics import classification_report

def test_model(model, data):
    # Extract train and test sets from passed data variable
    train, test = data

    # Extract features and targets for train and test sets respectively
    X_train, y_train = train
    X_test, y_test = test

    # Obtain train and test classification resuilts
    train_results = classification_report(y_train, model.predict(X_train))
    test_results = classification_report(y_test, model.predict(X_test))

    print("="*30, "Train Report", "="*30, "\n")
    print(train_results)

    print("="*30, "Test Report", "="*30, "\n")
    print(test_results)

    return

# Assemble data for testing
data_ = [[X_train, y_train], [X_test, y_test]]

"""### Evaluation for Logistic Regression model"""

# Linear model
test_model(log_pipe, data_)

"""### Evaluation for ExtraTrees model"""

# ExtraTrees model
test_model(extra_pipe, data_)

"""### Evaluation for CatBoost model"""

# CatBoost model
test_model(cat_pipe, data_)

"""Based on our final observations, the AdaBoost model seems to do the best in terms of magnitudes. However, in terms of generalization, the logistic regression and CatBoost models have it beat with ~1.0% and ~5.0% errors respectively.

Purely based off of generalization errors, the logistic regression model beats the others out completely. For this reason, a decision can be made to go for it.

The logistic regression model can be difficult to tune, as it has very few hyperparameters. The CatBoost model, on the other hand has a good number of hyperparameters that can be tuned to not only improve performance, but also reduce generalization error. As such, the CatBoost model is also a good fit.

### Hyperparametric Optimization

To better improve the generalization errors, we can attempt to optimize the parameters of the logistic regression and CatBoost models.
"""

from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold

# New LogisticRegression and CatBoost pipelines
new_cat_pipe = Pipeline(
    steps = [
        ("1", get_transformer(categorical_columns, scale_columns, one_hot=False)),
        ("2", CatBoostClassifier())
    ]
)

new_log_pipe = Pipeline(
    steps = [
        ("1", get_transformer(categorical_columns, scale_columns, one_hot=True)),
        ("2", LogisticRegression(max_iter = 1000))
    ]
)

# Hyperparametric grid
cat_param_grid = {
    # "2__bootstrap_type": ["Bayesian", "Bernoulli"],
    "2__l2_leaf_reg": [0., .1, .2, .3, .4, .5],
    "2__sampling_frequency": ["PerTree", "PerTreeLevel"]
}
log_param_grid = {
    # "2__bootstrap_type": ["Bayesian", "Bernoulli"],
    "2__penalty": ['l1', 'l2', 'elasticnet', None],
    "2__C": [1.0, 2., 3., 4.,],
    "2__solver": ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky',]
}

# Instantiate grid object
cat_grid = GridSearchCV(
    estimator = new_cat_pipe, param_grid = cat_param_grid,
    scoring = "accuracy", cv = RepeatedStratifiedKFold(n_splits = 2, n_repeats=3)
)
log_grid = GridSearchCV(
    estimator = new_log_pipe, param_grid = log_param_grid,
    scoring = "accuracy", cv = RepeatedStratifiedKFold(n_splits = 2, n_repeats=3)
)

# Fit grid on data
cat_grid.fit(X_train, y_train)

# Fit grid on data
log_grid.fit(X_train, y_train)

# Best estimator from logistic grid
test_model(log_grid.best_estimator_, data_)

# Bets estmator from CatBoost grid
test_model(cat_grid.best_estimator_, data_)

"""Based on our final observations from the optimization process, we have not been able to improve the models. This may be due to:

1. Insufficient data
2. Focusing on a small/wrong range of hyperparameters.
3. Focusing on the wrong hyperparameters.

Further optimization of the models might result in better models. This can be explored further. At this point, either model performs quite adequately. The final choice would be the Logistic Model, due to:

1. Lower generalization error,
2. Faster training, and
3. More explanability

<div align="center"><h3>5.1. Enter Model Evaluation Code Here</h3></div>

<div><h3>06. Model Deployment</h3></div>

With the model performance up to speed, the final artefact needs to be deployed for use. There are a number of options. They include:


1. Local Deployment
   * Flask
   * Django
2. Cloud Deployment
   * AWS
   * GCP
   * Streamlit + GitHub

<div align="center"><h3>6.1. Model Deployment Code</h3></div>
"""



















































































































































